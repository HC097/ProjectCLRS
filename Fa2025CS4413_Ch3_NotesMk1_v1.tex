\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, geometry, graphicx}
\usepackage{titlesec}
% For nicely formatted pseudocode:
\usepackage{algorithm}      % floating "Algorithm" environment
\usepackage{algpseudocode}  % modern algorithmic macros: \For, \While, \State, \To, etc.
\usepackage{float}          % enables the [H] placement specifier

% (Optional) nicer inline comments in pseudocode:
\algrenewcommand\algorithmiccomment[1]{\hfill\(\triangleright\)~#1}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{calculative}[theorem]{Calculative}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\titleformat{\section}[block]{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}[runin]{\bfseries}{}{0pt}{}[.]

\begin{document}

\begin{center}
\Large\textbf{Lecture 2 - Chpt 3: Characterizing Runing Times} \\
\large Harley Caham Combest \\
\large Fa2025 CS4413 Lecture Notes – Mk1
\end{center}

\vspace{1em}

\dotfill

\section*{Chapter 3: Characterizing Running Times}

\dotfill

\subsection*{3.1 O-notation, $\Omega$-notation, and $\Theta$-notation\vspace{1em}\\}

\noindent
\textbf{Historical Context.}  
The order of growth of an algorithm’s running time provides a simple and powerful way to characterize efficiency. Constants and lower-order terms become irrelevant for sufficiently large $n$, leaving only the asymptotic behavior. This abstraction lets us compare algorithms meaningfully, independent of machine or implementation details.  \\

\noindent
\textbf{Asymptotic Efficiency.}  
When input sizes are large, the multiplicative constants and lower-order terms of exact running times are dominated by the input size itself.  
\begin{itemize}
    \item For example, insertion sort runs in $\Theta(n^2)$ time in the worst case, while merge sort runs in $\Theta(n \log n)$. For large enough $n$, merge sort will always outperform insertion sort.  
    \item Thus, we focus on \emph{asymptotic notation}, which captures growth rates and allows precise comparison.  
\end{itemize}

\begin{algorithm}[H]
\caption{Insertion-Sort($A, n$)}
\begin{algorithmic}[1]
\For{$i = 2$ \textbf{to} $n$}
  \State $key \gets A[i]$
  \State $j \gets i - 1$
  \While{$j > 0$ \textbf{and} $A[j] > key$}
    \State $A[j+1] \gets A[j]$
    \State $j \gets j - 1$
  \EndWhile
  \State $A[j+1] \gets key$
\EndFor
\end{algorithmic}
\end{algorithm}

\noindent
\textbf{Analysis of Insertion Sort.}  
\begin{itemize}
    \item The \emph{outer loop} executes $n-1$ times.  
    \item The \emph{inner loop} executes at most $i-1$ times on iteration $i$.  
    \item Total number of inner-loop iterations is bounded above by $(n-1)(n-1) < n^2$. Each iteration costs constant time, so the running time is $O(n^2)$.  
\end{itemize}

\noindent
\textbf{Lower Bound Argument.}  
To show a $\Omega(n^2)$ lower bound, suppose the input of size $n$ has its largest $n/3$ elements in the first $n/3$ positions.  
\begin{itemize}
    \item Each of these values must move into the last $n/3$ positions.  
    \item To do so, each must pass through the middle $n/3$ positions, one step at a time.  
    \item Hence at least $(n/3)(n/3) = n^2/9$ moves are required.  
\end{itemize}
Thus, the worst-case running time of insertion sort is $\Omega(n^2)$.  

\begin{proposition}
Since insertion sort is $O(n^2)$ in all cases and $\Omega(n^2)$ in the worst case, its worst-case running time is $\Theta(n^2)$.  
\end{proposition}

\noindent
\textbf{Concluding Remark.}  
Asymptotic notations abstract away machine-dependent constants, exposing the essential growth behavior. Insertion sort demonstrates $\Theta(n^2)$ worst-case time, while merge sort’s $\Theta(n \log n)$ makes it asymptotically more efficient. These notations form the foundation for algorithm analysis.  

\newpage

\subsection*{3.2 Asymptotic notation: formal definitions\vspace{1em}\\}

\noindent
\textbf{Motivation.}  
The informal use of asymptotic notation in $\S 3.1$ allows us to discard lower-order terms and constants. We now give precise mathematical definitions, expressed in terms of sets of functions.  

\begin{center}
\includegraphics[scale=0.25]{Fig3pt2.png}
\end{center}

\begin{definition}[Big-O Notation]
For a given function $g(n)$, we define
\[
O(g(n)) = \{ f(n) : \exists \, c > 0, \, n_0 > 0 \text{ such that } 0 \leq f(n) \leq c g(n) \text{ for all } n \geq n_0 \}.
\]
We say $f(n) = O(g(n))$ if $f(n)$ grows no faster than $g(n)$, up to constant factors.
\end{definition}

\begin{definition}[Big-$\Omega$ Notation]
\[
\Omega(g(n)) = \{ f(n) : \exists \, c > 0, \, n_0 > 0 \text{ such that } 0 \leq c g(n) \leq f(n) \text{ for all } n \geq n_0 \}.
\]
We say $f(n) = \Omega(g(n))$ if $f(n)$ grows at least as fast as $g(n)$, up to constant factors.
\end{definition}

\begin{definition}[Big-$\Theta$ Notation]
\[
\Theta(g(n)) = \{ f(n) : \exists \, c_1, c_2 > 0, \, n_0 > 0 \text{ such that } 0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \text{ for all } n \geq n_0 \}.
\]
We say $f(n) = \Theta(g(n))$ if $f(n)$ grows precisely at the same asymptotic rate as $g(n)$, up to constant factors.
\end{definition}

\begin{theorem}[Equivalence of $\Theta$-notation]
For functions $f(n)$ and $g(n)$,
\[
f(n) = \Theta(g(n)) \quad \iff \quad f(n) = O(g(n)) \ \text{and} \ f(n) = \Omega(g(n)).
\]
\end{theorem}

\begin{definition}[Little-$o$ Notation]
\[
o(g(n)) = \{ f(n) : \forall c > 0, \, \exists n_0 > 0 \ \text{such that } 0 \leq f(n) < c g(n) \ \text{for all } n \geq n_0 \}.
\]
We say $f(n) = o(g(n))$ if $f(n)$ grows strictly slower than $g(n)$, i.e.\ becomes insignificant relative to $g(n)$ as $n \to \infty$.
\end{definition}

\begin{definition}[Little-$\omega$ Notation]
\[
\omega(g(n)) = \{ f(n) : \forall c > 0, \, \exists n_0 > 0 \ \text{such that } 0 \leq c g(n) < f(n) \ \text{for all } n \geq n_0 \}.
\]
We say $f(n) = \omega(g(n))$ if $f(n)$ grows strictly faster than $g(n)$, i.e.\ eventually dominates $g(n)$ as $n \to \infty$.
\end{definition}

\noindent
\textbf{Examples.}  
\begin{itemize}
    \item $4n^2 + 100n + 500 = O(n^2)$, since constants and lower-order terms can be absorbed.  
    \item The same function is also $\Omega(n^2)$, hence $\Theta(n^2)$.  
    \item $2n = o(n^2)$ because $2n/n^2 \to 0$ as $n \to \infty$.  
    \item $n^2 = \omega(n)$ because $n^2/n \to \infty$.  
\end{itemize}

\noindent
\textbf{Asymptotic Notation in Equations.}  
We often use equality signs for membership or shorthand:
\[
2n^2 + 3n + 1 = 2n^2 + \Theta(n) = \Theta(n^2).
\]
Interpretation: the term $\Theta(n)$ denotes some anonymous function of order $n$, and the final simplification captures the dominant order.

\noindent
\textbf{Properties of Asymptotic Notation.}  
For asymptotically positive functions $f, g, h$:  
\begin{itemize}
    \item \textit{Transitivity:}  
    $f = \Theta(g)$ and $g = \Theta(h) \implies f = \Theta(h)$;  
    similarly for $O$, $\Omega$, $o$, $\omega$.  
    \item \textit{Reflexivity:} $f = O(f)$, $f = \Omega(f)$, $f = \Theta(f)$.  
    \item \textit{Symmetry:} $f = \Theta(g) \iff g = \Theta(f)$.  
    \item \textit{Transpose symmetry:} $f = O(g) \iff g = \Omega(f)$, and $f = o(g) \iff g = \omega(f)$.  
\end{itemize}

\noindent
\textbf{Non-comparability.}  
Not all functions are asymptotically comparable. For example, $n$ and $n^{1+\sin n}$ cannot be related by $O, \Omega$, or $\Theta$, since the exponent oscillates between $0$ and $2$.  

\noindent
\textbf{Proper Abuses of Notation.}  
\begin{itemize}
    \item Writing $f(n) = O(g(n))$ instead of $f(n) \in O(g(n))$.  
    \item Omitting the variable tending to infinity (e.g.\ $O(1)$).  
    \item Writing $T(n) = O(1)$ for $n < 3$, interpreted as “bounded by a constant” for small $n$.  
    \item Using asymptotic notation even when a function is defined only on a restricted domain (e.g.\ $n$ a power of $2$).  
\end{itemize}

\noindent
\textbf{Concluding Remark.}  
Asymptotic notation formalizes the intuitive act of discarding inessential details. Big notations give loose or tight bounds, while little notations capture strict dominance. The rules and conventions ensure consistency, while accepted abuses keep the notation concise without losing correctness.  

\newpage

\subsection*{3.3 Standard notations and common functions \vspace{1em}\\}

\noindent
\textbf{Motivation.}  
When analyzing algorithms, certain functions arise repeatedly. This section reviews their definitions, properties, and asymptotic growth, providing a toolkit for later proofs and comparisons.

\subsubsection*{Monotonicity}
\begin{definition}[Monotonicity]
A function $f(n)$ is:
\begin{itemize}
    \item \emph{Monotonically increasing} if $m \leq n \implies f(m) \leq f(n)$.  
    \item \emph{Monotonically decreasing} if $m \leq n \implies f(m) \geq f(n)$.  
    \item \emph{Strictly increasing} if $m < n \implies f(m) < f(n)$.  
    \item \emph{Strictly decreasing} if $m < n \implies f(m) > f(n)$.  
\end{itemize}
\end{definition}

\subsubsection*{Floors and Ceilings}
\begin{definition}[Floor and Ceiling]
For real $x$:  
\[
\lfloor x \rfloor = \max \{ m \in \mathbb{Z} : m \leq x \}, \quad
\lceil x \rceil = \min \{ m \in \mathbb{Z} : m \geq x \}.
\]
\end{definition}
Properties:  
\[
\lfloor x \rfloor \leq x \leq \lceil x \rceil, \qquad
\lfloor x \rfloor + \lceil x \rceil = \lfloor 2x \rfloor \ \text{or} \ \lfloor 2x \rfloor + 1.
\]

\subsubsection*{Modular Arithmetic}
\begin{definition}[Modulo]
For integers $a$ and $n > 0$, the value $a \bmod n$ is the remainder of $a / n$.  
\end{definition}
We say $a \equiv b \pmod n$ if $n \mid (a - b)$, i.e.\ $a$ and $b$ leave the same remainder upon division by $n$.  

\subsubsection*{Polynomials}
A polynomial of degree $d$ is
\[
p(n) = a_d n^d + a_{d-1} n^{d-1} + \cdots + a_0, \quad a_d \neq 0.
\]
If $a_d > 0$, then $p(n)$ is asymptotically positive and
\[
p(n) = \Theta(n^d).
\]
A function is \emph{polynomially bounded} if $f(n) = O(n^k)$ for some constant $k$.

\subsubsection*{Exponentials}
For $a > 0$ and integers $m, n$:
\[
a^0 = 1, \quad a^1 = a, \quad a^{-1} = \tfrac{1}{a}, \quad (a^m)^n = a^{mn}, \quad a^m a^n = a^{m+n}.
\]
If $a > 1$, $a^n$ is monotonically increasing in $n$.  
\begin{proposition}
For all real $a > 1$ and $b$, we have
\[
n^b = o(a^n).
\]
Thus, every exponential outgrows every polynomial.
\end{proposition}

\subsubsection*{Logarithms}
We use:
\[
\lg n = \log_2 n, \quad \ln n = \log_e n, \quad \lg^k n = (\lg n)^k, \quad \lg \lg n = \lg(\lg n).
\]
Properties:  
\[
\log_b a = \frac{\log_c a}{\log_c b}, \quad \text{for } a,b,c > 0, \ b,c \neq 1.
\]
Thus, base choice only changes constants in asymptotic analysis.  
\begin{definition}[Polylogarithmic bound]
A function is polylogarithmically bounded if $f(n) = O(\lg^k n)$ for some constant $k$.
\end{definition}

\subsubsection*{Factorials}
\[
n! = 1 \cdot 2 \cdot 3 \cdots n, \quad 0! = 1.
\]
A weak bound: $n! \leq n^n$.  
Stronger bounds use Stirling’s approximation:
\[
n! \sim \sqrt{2 \pi n} \left(\tfrac{n}{e}\right)^n.
\]
Hence, $n!$ grows faster than polynomial but slower than $n^n$.  

\subsubsection*{Functional Iteration}
For function $f(n)$ and nonnegative integer $i$:
\[
f^{(0)}(n) = n, \quad f^{(i)}(n) = f(f^{(i-1)}(n)).
\]
Example: if $f(n) = 2n$, then $f^{(i)}(n) = 2^i n$.

\subsubsection*{Iterated Logarithm}
\begin{definition}[Iterated logarithm]
\[
\lg^* n = \min \{ i \geq 0 : \lg^{(i)} n \leq 1 \}.
\]
\end{definition}
Values: $\lg^* 2 = 1$, $\lg^* 4 = 2$, $\lg^* 16 = 3$, $\lg^* 65536 = 4$.  
Even for astronomically large $n$, $\lg^* n \leq 5$.  

\subsubsection*{Fibonacci Numbers}
Defined recursively:
\[
F_0 = 0, \quad F_1 = 1, \quad F_i = F_{i-1} + F_{i-2} \quad (i \geq 2).
\]
Sequence: $0, 1, 1, 2, 3, 5, 8, 13, \dots$  

\noindent
\textbf{Relation to Golden Ratio.}  
Let $\varphi = \tfrac{1+\sqrt{5}}{2}$ and $\hat{\varphi} = \tfrac{1-\sqrt{5}}{2}$. Then
\[
F_i = \frac{\varphi^i - \hat{\varphi}^i}{\sqrt{5}}.
\]
This shows $F_i$ grows asymptotically as $\Theta(\varphi^i)$.  

\noindent
\textbf{Concluding Remark.}  
These standard functions — polynomials, exponentials, logarithms, factorials, and Fibonacci numbers — provide the basic comparative vocabulary for asymptotic growth. Their properties, especially monotonicity and bounding relationships, are essential in algorithm analysis.  


\end{document}